# RecurrNet_Sp22

This repository provides all relevant files to re-create a neural network simulation of visual recognition of complex words, specifically how this task is accomplished by the human brain, for my PhD dissertation. In this project I trained a recurrent neural network on artificial languages to simulate empirical findings with adult and child readers investigating the dynamics of morphological processing during visual word recognition across languages. More details on the project can be found in the included report, "Chapter14_StevensDissertation.pdf". Network training and testing was conducted using Lens (examples, instructions for use and software download can be found here: https://ni.cmu.edu/~plaut/Lens/Manual/) and output analyses were done using R Version 4.3.1 (can be downloaded from https://www.r-project.org/).

The first two folders, "14t4i2o_language" and "8t6i6o_language", contain files relevant to network training for a more morphologically rich langage (14t4i2o-language) and one for a more morphologically impoverished language (8t6i6o). These languages are thus named because the rich language contains words from 14 transparent stems, 4 intermediate stems and 2 opaque stems, while the impoverished language contrains words from 8 transparent stems, 6 intermediate stems and 6 opaque stems. Transparent stems combine with suffixes to form predictable meanings (TRUSTING, TRUSTEE, TRUSTWORTHY) while opaque stems form words with less obvious meanings (WHISKER, WHISKY, WHISKING). See report for more information. 

Each language folder contains a network file (".in", detailing the structure of the neural network), example files (".ex", detailing the information presented to the network for training), weight files (".wt.bz2", providing the learned weight values of the network at three points during training; see report for details on why certain timepoints were used, p 84-85), and an error report (.png, showing the decline in error over training). The .in and .ex files can be used to re-run training, or the .wt.bz2 files can be used to forego training (which took several weeks on a multi-node cluster) and move on to network testing and analyses. 

The second two folders, "testing_14t4i2o_language" and "testing_8t6i6o_language", include the network structure used for testing (".in"), as well as example files (".ex") used for testing at 1, 3, 5, 7, and 9 "ticks" (a Lens unit for network time) of prime exposure. The example file structures for 2, 4, 6, 8, and 10 ticks, which were also used in the report, can be extrapolated if required, but were excluded for the sake of storage space. "wts2outs.tcl" automates the procedure for testing the network on all example files, using weights from all selected time points from training. Running "wts2outs.tcl" will result in the generation of 15 .out files providing activation values of the hidden and output layers over the course of each testing example, characterizing the network's response to testing examples at different points in training.

The "analysis_code" folder contains R scripts for analyzing and visualizing the results of testing, to recreate the plots and statistical test results provided in the included report. Run "primetest_analyze" first. This generates the "outdf_train...csv" files to be read in by "primetest_visualize".
